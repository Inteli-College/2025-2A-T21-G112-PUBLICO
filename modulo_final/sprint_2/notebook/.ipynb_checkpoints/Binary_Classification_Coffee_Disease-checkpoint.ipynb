{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Sprint 2 - Classifica√ß√£o Bin√°ria de Doen√ßas em Folhas de Caf√©\n",
    "\n",
    "Este notebook implementa um modelo de classifica√ß√£o bin√°ria para detectar se uma folha de caf√© √©:\n",
    "- **Saud√°vel (Healthy)**: Classe 0\n",
    "- **N√£o Saud√°vel (Doente)**: Classe 1 (todas as doen√ßas combinadas)\n",
    "\n",
    "## Caracter√≠sticas do Modelo:\n",
    "- **Classifica√ß√£o Bin√°ria**: Simplifica o problema para aumentar a acur√°cia\n",
    "- **Data Augmentation Avan√ßada**: T√©cnicas de aumento de dados\n",
    "- **Arquiteturas Customizadas**: CNNs otimizadas para classifica√ß√£o bin√°ria\n",
    "- **Learning Rate Scheduling**: Ajuste din√¢mico da taxa de aprendizado\n",
    "- **Early Stopping**: Preven√ß√£o de overfitting\n",
    "- **M√©tricas Detalhadas**: Precision, Recall, F1-Score para cada classe\n",
    "- **Visualiza√ß√£o de Steps**: Progresso detalhado durante o treinamento\n",
    "\n",
    "## Classes Originais Convertidas:\n",
    "- **Healthy** ‚Üí Saud√°vel (0)\n",
    "- **Cerscospora** ‚Üí N√£o Saud√°vel (1)\n",
    "- **Leaf rust** ‚Üí N√£o Saud√°vel (1)\n",
    "- **Miner** ‚Üí N√£o Saud√°vel (1)\n",
    "- **Phoma** ‚Üí N√£o Saud√°vel (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Download dos Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dos Datasets do Kaggle\n",
    "import kagglehub\n",
    "\n",
    "print(\"üì• Baixando datasets de doen√ßas em folhas de caf√©...\")\n",
    "\n",
    "# Download dos datasets\n",
    "noamaanabdulazeem_jmuben_coffee_dataset_path = kagglehub.dataset_download('noamaanabdulazeem/jmuben-coffee-dataset')\n",
    "gauravduttakiit_coffee_leaf_diseases_path = kagglehub.dataset_download('gauravduttakiit/coffee-leaf-diseases')\n",
    "biniyamyoseph_ethiopian_coffee_leaf_disease_path = kagglehub.dataset_download('biniyamyoseph/ethiopian-coffee-leaf-disease')\n",
    "mohammedzwaughfa_coffee_leaf_disease_dataset_path = kagglehub.dataset_download('mohammedzwaughfa/coffee-leaf-disease-dataset')\n",
    "\n",
    "print('‚úÖ Download dos datasets conclu√≠do!')\n",
    "print(f\"Dataset 1: {noamaanabdulazeem_jmuben_coffee_dataset_path}\")\n",
    "print(f\"Dataset 2: {gauravduttakiit_coffee_leaf_diseases_path}\")\n",
    "print(f\"Dataset 3: {biniyamyoseph_ethiopian_coffee_leaf_disease_path}\")\n",
    "print(f\"Dataset 4: {mohammedzwaughfa_coffee_leaf_disease_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Importa√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, StepLR\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Configura√ß√£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Binary Classification\n",
    "CONFIG = {\n",
    "    'img_size': 224,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 10,  # For early stopping\n",
    "    'min_delta': 0.001,  # Minimum change to qualify as improvement\n",
    "    'subset_fraction': 0.8,  # Use 80% of data\n",
    "    \n",
    "    # Data paths\n",
    "    'save_dir': './modulo_final/sprint_2/modelos',  # For saving models\n",
    "    \n",
    "    # Class information - BINARY CLASSIFICATION\n",
    "    'class_names': ['Saud√°vel', 'N√£o Saud√°vel'],\n",
    "    'num_classes': 2,\n",
    "    \n",
    "    # Model configurations\n",
    "    'models_to_train': ['BinaryCNN_Light', 'BinaryCNN_Deep', 'BinaryCNN_Efficient'],\n",
    "    \n",
    "    # Training parameters\n",
    "    'use_mixed_precision': True,\n",
    "    'gradient_clip_norm': 1.0,\n",
    "    'scheduler_type': 'cosine',\n",
    "    \n",
    "    # Data augmentation\n",
    "    'use_advanced_augmentation': True,\n",
    "    'mixup_alpha': 0.2,\n",
    "    'cutmix_alpha': 1.0,\n",
    "    'label_smoothing': 0.1\n",
    "}\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"Classes: {CONFIG['class_names']}\")\n",
    "print(f\"Models to train: {CONFIG['models_to_train']}\")\n",
    "print(f\"Save directory: {CONFIG['save_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Dataset Customizado para Classifica√ß√£o Bin√°ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper that converts multi-class to binary classification\"\"\"\n",
    "    def __init__(self, original_dataset, healthy_class_idx=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_dataset: The original multi-class dataset\n",
    "            healthy_class_idx: Index of the 'Healthy' class in original dataset\n",
    "        \"\"\"\n",
    "        self.dataset = original_dataset\n",
    "        self.healthy_class_idx = healthy_class_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        # Convert to binary: 0 = Healthy, 1 = Not Healthy\n",
    "        binary_label = 0 if label == self.healthy_class_idx else 1\n",
    "        return image, binary_label\n",
    "\n",
    "print(\"Binary dataset class created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Data Augmentation\n",
    "class AdvancedTransforms:\n",
    "    \"\"\"Advanced data augmentation techniques for better model generalization\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_train_transforms(img_size=224):\n",
    "        \"\"\"Get training transforms with advanced augmentation\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size + 32, img_size + 32)),\n",
    "            transforms.RandomCrop(img_size),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.2),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            transforms.RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_val_transforms(img_size=224):\n",
    "        \"\"\"Get validation transforms (minimal augmentation)\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Mixup and CutMix implementations\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"Apply mixup augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup loss function\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"Apply CutMix augmentation\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    y_a, y_b = y, y[index]\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    # Adjust lambda\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    \"\"\"Generate random bounding box for CutMix\"\"\"\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "print(\"Advanced data augmentation functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Arquiteturas de CNN para Classifica√ß√£o Bin√°ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary CNN Architectures\n",
    "class BinaryCNN_Light(nn.Module):\n",
    "    \"\"\"Lightweight Binary CNN - Fast and efficient\"\"\"\n",
    "    def __init__(self):\n",
    "        super(BinaryCNN_Light, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 2)  # Binary classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class BinaryCNN_Deep(nn.Module):\n",
    "    \"\"\"Deeper Binary CNN with residual connections\"\"\"\n",
    "    def __init__(self):\n",
    "        super(BinaryCNN_Deep, self).__init__()\n",
    "        \n",
    "        # Use ResNet-like structure\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 2)  # Binary classification\n",
    "        )\n",
    "    \n",
    "    def _make_layer(self, inplanes, planes, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(inplanes, planes, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(planes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic residual block\"\"\"\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inplanes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class BinaryCNN_Efficient(nn.Module):\n",
    "    \"\"\"Efficient Binary CNN - Balanced speed and accuracy\"\"\"\n",
    "    def __init__(self):\n",
    "        super(BinaryCNN_Efficient, self).__init__()\n",
    "        \n",
    "        # Depthwise separable convolutions for efficiency\n",
    "        self.features = nn.Sequential(\n",
    "            # Stem\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Depthwise separable blocks\n",
    "            self._depthwise_separable(32, 64, stride=1),\n",
    "            self._depthwise_separable(64, 128, stride=2),\n",
    "            self._depthwise_separable(128, 256, stride=2),\n",
    "            self._depthwise_separable(256, 512, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 2)  # Binary classification\n",
    "        )\n",
    "    \n",
    "    def _depthwise_separable(self, in_channels, out_channels, stride=1):\n",
    "        return nn.Sequential(\n",
    "            # Depthwise\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, \n",
    "                     padding=1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Pointwise\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"Binary CNN architectures loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Carregamento e Prepara√ß√£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "DATASET_PATHS = [\n",
    "    os.path.join(mohammedzwaughfa_coffee_leaf_disease_dataset_path, \"dataset/test\"),\n",
    "    os.path.join(gauravduttakiit_coffee_leaf_diseases_path, \"train\"),\n",
    "    os.path.join(gauravduttakiit_coffee_leaf_diseases_path, \"test\"),\n",
    "    os.path.join(biniyamyoseph_ethiopian_coffee_leaf_disease_path, \"ethiopian cofee leaf dataset/train aug\"),\n",
    "    os.path.join(biniyamyoseph_ethiopian_coffee_leaf_disease_path, \"ethiopian cofee leaf dataset/test\"),\n",
    "    os.path.join(noamaanabdulazeem_jmuben_coffee_dataset_path, \"JMuBEN\"),\n",
    "]\n",
    "\n",
    "print(\"üîç Loading and preparing datasets for binary classification...\")\n",
    "\n",
    "# Load datasets with transforms\n",
    "train_transform = AdvancedTransforms.get_train_transforms(CONFIG['img_size'])\n",
    "val_transform = AdvancedTransforms.get_val_transforms(CONFIG['img_size'])\n",
    "\n",
    "def load_datasets(paths, transform):\n",
    "    \"\"\"Load all datasets and combine them\"\"\"\n",
    "    datasets_list = []\n",
    "    for path in paths:\n",
    "        if os.path.exists(path):\n",
    "            ds = datasets.ImageFolder(path, transform=transform)\n",
    "            datasets_list.append(ds)\n",
    "            print(f\"  Loaded: {path.split('/')[-1]} - {len(ds)} samples\")\n",
    "    return ConcatDataset(datasets_list) if datasets_list else None\n",
    "\n",
    "# Load combined dataset\n",
    "combined_dataset = load_datasets(DATASET_PATHS, train_transform)\n",
    "\n",
    "if combined_dataset is None:\n",
    "    raise ValueError(\"No datasets were loaded. Please check the dataset paths.\")\n",
    "\n",
    "# Find the index of 'Healthy' class in the original dataset\n",
    "# Assuming the first dataset has the class structure\n",
    "sample_dataset = datasets.ImageFolder(DATASET_PATHS[0], transform=train_transform)\n",
    "original_classes = sample_dataset.classes\n",
    "print(f\"\\nOriginal classes: {original_classes}\")\n",
    "\n",
    "# Find healthy class index (usually 'Healthy' or similar)\n",
    "healthy_idx = None\n",
    "for idx, class_name in enumerate(original_classes):\n",
    "    if 'healthy' in class_name.lower():\n",
    "        healthy_idx = idx\n",
    "        print(f\"Found 'Healthy' class at index {healthy_idx}\")\n",
    "        break\n",
    "\n",
    "if healthy_idx is None:\n",
    "    print(\"Warning: 'Healthy' class not found. Using index 1 as default.\")\n",
    "    healthy_idx = 1\n",
    "\n",
    "# Convert to binary dataset\n",
    "binary_dataset = BinaryDataset(combined_dataset, healthy_class_idx=healthy_idx)\n",
    "\n",
    "# Use subset for faster training\n",
    "subset_size = int(len(binary_dataset) * CONFIG['subset_fraction'])\n",
    "binary_dataset, _ = random_split(binary_dataset, [subset_size, len(binary_dataset) - subset_size])\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.7 * len(binary_dataset))\n",
    "val_size = len(binary_dataset) - train_size\n",
    "\n",
    "train_data, val_data = random_split(binary_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset preparation completed!\")\n",
    "print(f\"Total samples: {len(binary_dataset)}\")\n",
    "print(f\"Training samples: {train_size}\")\n",
    "print(f\"Validation samples: {val_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"\\nBinary classes: {CONFIG['class_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 8. Fun√ß√µes de Treinamento e Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "class MetricsTracker:\n",
    "    \"\"\"Track training metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        self.train_precisions = []\n",
    "        self.val_precisions = []\n",
    "        self.train_recalls = []\n",
    "        self.val_recalls = []\n",
    "        self.train_f1s = []\n",
    "        self.val_f1s = []\n",
    "        self.learning_rates = []\n",
    "        self.epochs = []\n",
    "    \n",
    "    def update(self, epoch, train_loss, val_loss, train_metrics, val_metrics, lr):\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_accuracies.append(train_metrics['accuracy'])\n",
    "        self.val_accuracies.append(val_metrics['accuracy'])\n",
    "        self.train_precisions.append(train_metrics['precision'])\n",
    "        self.val_precisions.append(val_metrics['precision'])\n",
    "        self.train_recalls.append(train_metrics['recall'])\n",
    "        self.val_recalls.append(val_metrics['recall'])\n",
    "        self.train_f1s.append(train_metrics['f1'])\n",
    "        self.val_f1s.append(val_metrics['f1'])\n",
    "        self.learning_rates.append(lr)\n",
    "    \n",
    "    def get_best_epoch(self):\n",
    "        best_idx = np.argmax(self.val_accuracies)\n",
    "        return self.epochs[best_idx], self.val_accuracies[best_idx]\n",
    "\n",
    "def calculate_metrics(all_preds, all_targets):\n",
    "    \"\"\"Calculate binary classification metrics\"\"\"\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_preds, average='binary', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy * 100,\n",
    "        'precision': precision * 100,\n",
    "        'recall': recall * 100,\n",
    "        'f1': f1 * 100\n",
    "    }\n",
    "\n",
    "def train_and_evaluate_model(model, model_name, train_loader, val_loader, config):\n",
    "    \"\"\"Train and evaluate binary classification model with detailed progress\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîß Training: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    metrics = MetricsTracker()\n",
    "    early_stopping = EarlyStopping(patience=config['patience'], min_delta=config['min_delta'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    \n",
    "    if config['scheduler_type'] == 'cosine':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=config['num_epochs'])\n",
    "    elif config['scheduler_type'] == 'plateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "    else:\n",
    "        scheduler = StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler() if config['use_mixed_precision'] and torch.cuda.is_available() else None\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"\\nüìä Model Configuration:\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Epochs: {config['num_epochs']}\")\n",
    "    print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "    print(f\"  Batch size: {config['batch_size']}\")\n",
    "    print(f\"  Scheduler: {config['scheduler_type']}\")\n",
    "    print(f\"  Mixed precision: {config['use_mixed_precision']}\")\n",
    "    print(f\"\\nüöÄ Starting training...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n",
    "        for batch_idx, (data, target) in enumerate(train_pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Apply augmentation\n",
    "            if config['use_advanced_augmentation'] and np.random.random() < 0.5:\n",
    "                if np.random.random() < 0.5:\n",
    "                    data, target_a, target_b, lam = mixup_data(data, target, config['mixup_alpha'])\n",
    "                    if scaler:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            output = model(data)\n",
    "                            loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "                    else:\n",
    "                        output = model(data)\n",
    "                        loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "                else:\n",
    "                    data, target_a, target_b, lam = cutmix_data(data, target, config['cutmix_alpha'])\n",
    "                    if scaler:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            output = model(data)\n",
    "                            loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "                    else:\n",
    "                        output = model(data)\n",
    "                        loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "            else:\n",
    "                if scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output = model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                else:\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "            \n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_norm'])\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip_norm'])\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            all_train_preds.extend(predicted.cpu().numpy())\n",
    "            all_train_targets.extend(target.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_targets = []\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Val]  \")\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_pbar:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_targets.extend(target.cpu().numpy())\n",
    "                \n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_metrics = calculate_metrics(all_train_preds, all_train_targets)\n",
    "        val_metrics = calculate_metrics(all_val_preds, all_val_targets)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        metrics.update(epoch, train_loss, val_loss, train_metrics, val_metrics, current_lr)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']} Summary:\")\n",
    "        print(f\"{'‚îÄ'*80}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train Acc: {train_metrics['accuracy']:.2f}% | Val Acc: {val_metrics['accuracy']:.2f}%\")\n",
    "        print(f\"Train Prec: {train_metrics['precision']:.2f}% | Val Prec: {val_metrics['precision']:.2f}%\")\n",
    "        print(f\"Train Recall: {train_metrics['recall']:.2f}% | Val Recall: {val_metrics['recall']:.2f}%\")\n",
    "        print(f\"Train F1: {train_metrics['f1']:.2f}% | Val F1: {val_metrics['f1']:.2f}%\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        print(f\"{'‚îÄ'*80}\\n\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        if config['scheduler_type'] == 'plateau':\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"\\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    best_epoch, best_val_acc = metrics.get_best_epoch()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úÖ Training Completed!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total training time: {training_time/60:.2f} minutes\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch+1}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "print(\"Training and evaluation functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 9. Fun√ß√µes de Visualiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(metrics, model_name, save_path=None):\n",
    "    \"\"\"Plot comprehensive training curves for binary classification\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(f'{model_name} - Binary Classification Training Progress', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(metrics.epochs, metrics.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(metrics.epochs, metrics.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    axes[0, 1].plot(metrics.epochs, metrics.train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 1].plot(metrics.epochs, metrics.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 1].set_title('Accuracy Curves')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision curves\n",
    "    axes[0, 2].plot(metrics.epochs, metrics.train_precisions, 'b-', label='Training Precision', linewidth=2)\n",
    "    axes[0, 2].plot(metrics.epochs, metrics.val_precisions, 'r-', label='Validation Precision', linewidth=2)\n",
    "    axes[0, 2].set_title('Precision Curves')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Precision (%)')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recall curves\n",
    "    axes[1, 0].plot(metrics.epochs, metrics.train_recalls, 'b-', label='Training Recall', linewidth=2)\n",
    "    axes[1, 0].plot(metrics.epochs, metrics.val_recalls, 'r-', label='Validation Recall', linewidth=2)\n",
    "    axes[1, 0].set_title('Recall Curves')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Recall (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1-Score curves\n",
    "    axes[1, 1].plot(metrics.epochs, metrics.train_f1s, 'b-', label='Training F1-Score', linewidth=2)\n",
    "    axes[1, 1].plot(metrics.epochs, metrics.val_f1s, 'r-', label='Validation F1-Score', linewidth=2)\n",
    "    axes[1, 1].set_title('F1-Score Curves')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('F1-Score (%)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate curve\n",
    "    axes[1, 2].plot(metrics.epochs, metrics.learning_rates, 'g-', linewidth=2)\n",
    "    axes[1, 2].set_title('Learning Rate Schedule')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Learning Rate')\n",
    "    axes[1, 2].set_yscale('log')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Training curves saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, class_names, model_name, save_path=None):\n",
    "    \"\"\"Plot confusion matrix for binary classification\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(data_loader, desc=\"Calculating confusion matrix\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class 1\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Plot confusion matrix and ROC curve\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(f'{model_name} - Binary Classification Performance', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(all_targets, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    axes[1].set_xlim([0.0, 1.0])\n",
    "    axes[1].set_ylim([0.0, 1.05])\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].legend(loc=\"lower right\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Confusion matrix saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name} - Classification Report\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(classification_report(all_targets, all_preds, target_names=class_names))\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "def plot_model_comparison(all_metrics, save_path=None):\n",
    "    \"\"\"Plot comparison between different models\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Sprint 2 - Binary Classification Model Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    model_names = list(all_metrics.keys())\n",
    "    \n",
    "    # Best validation accuracy\n",
    "    best_accuracies = [np.max(metrics.val_accuracies) for metrics in all_metrics.values()]\n",
    "    bars = axes[0, 0].bar(model_names, best_accuracies, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    axes[0, 0].set_title('Best Validation Accuracy')\n",
    "    axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 0].set_ylim(0, 100)\n",
    "    for bar, acc in zip(bars, best_accuracies):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                        f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Best F1-Score\n",
    "    best_f1s = [np.max(metrics.val_f1s) for metrics in all_metrics.values()]\n",
    "    bars = axes[0, 1].bar(model_names, best_f1s, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    axes[0, 1].set_title('Best Validation F1-Score')\n",
    "    axes[0, 1].set_ylabel('F1-Score (%)')\n",
    "    axes[0, 1].set_ylim(0, 100)\n",
    "    for bar, f1 in zip(bars, best_f1s):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                        f'{f1:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy over time\n",
    "    for model_name, metrics in all_metrics.items():\n",
    "        axes[1, 0].plot(metrics.epochs, metrics.val_accuracies, label=model_name, linewidth=2)\n",
    "    axes[1, 0].set_title('Validation Accuracy Over Time')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss over time\n",
    "    for model_name, metrics in all_metrics.items():\n",
    "        axes[1, 1].plot(metrics.epochs, metrics.val_losses, label=model_name, linewidth=2)\n",
    "    axes[1, 1].set_title('Validation Loss Over Time')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Model comparison saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 10. Pipeline de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Pipeline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Starting Sprint 2 Binary Classification Training Pipeline\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Define binary models\n",
    "models_dict = {\n",
    "    \"BinaryCNN_Light\": BinaryCNN_Light(),\n",
    "    \"BinaryCNN_Deep\": BinaryCNN_Deep(),\n",
    "    \"BinaryCNN_Efficient\": BinaryCNN_Efficient(),\n",
    "}\n",
    "\n",
    "# Print model information\n",
    "print(\"üìä Model Information:\")\n",
    "print(\"‚îÄ\"*80)\n",
    "for name, model in models_dict.items():\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(\"‚îÄ\"*80 + \"\\n\")\n",
    "\n",
    "# Train all models\n",
    "results = {}\n",
    "all_metrics = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    try:\n",
    "        # Train model\n",
    "        trained_model, metrics = train_and_evaluate_model(\n",
    "            model, name, train_loader, val_loader, CONFIG\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = trained_model\n",
    "        all_metrics[name] = metrics\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(CONFIG['save_dir'], f'{name}_best.pth')\n",
    "        torch.save(trained_model.state_dict(), model_path)\n",
    "        print(f\"üíæ Model saved to {model_path}\\n\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        plot_training_curves(metrics, name, \n",
    "                           os.path.join(CONFIG['save_dir'], f'{name}_training_curves.png'))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(trained_model, val_loader, CONFIG['class_names'], name,\n",
    "                            os.path.join(CONFIG['save_dir'], f'{name}_confusion_matrix.png'))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error training {name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Training Pipeline Completed!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Trained models: {list(results.keys())}\")\n",
    "print(f\"Results saved to: {CONFIG['save_dir']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Plot model comparison\n",
    "if all_metrics:\n",
    "    plot_model_comparison(all_metrics, \n",
    "                        os.path.join(CONFIG['save_dir'], 'model_comparison.png'))\n",
    "\n",
    "# Final results summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä Final Results Summary\")\n",
    "print(\"=\"*80)\n",
    "for name, metrics in all_metrics.items():\n",
    "    best_epoch, best_acc = metrics.get_best_epoch()\n",
    "    best_f1_idx = np.argmax(metrics.val_f1s)\n",
    "    best_f1 = metrics.val_f1s[best_f1_idx]\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Best Accuracy: {best_acc:.2f}% at epoch {best_epoch+1}\")\n",
    "    print(f\"  Best F1-Score: {best_f1:.2f}% at epoch {best_f1_idx+1}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 11. Resumo e Conclus√µes\n",
    "\n",
    "### Caracter√≠sticas da Classifica√ß√£o Bin√°ria:\n",
    "\n",
    "1. **Simplifica√ß√£o do Problema**:\n",
    "   - Redu√ß√£o de 5 classes para 2 classes (Saud√°vel vs N√£o Saud√°vel)\n",
    "   - Aumento esperado na acur√°cia devido √† simplifica√ß√£o\n",
    "   - Mais adequado para aplica√ß√µes pr√°ticas de detec√ß√£o de doen√ßas\n",
    "\n",
    "2. **M√©tricas Espec√≠ficas**:\n",
    "   - **Precision**: Importante para minimizar falsos positivos\n",
    "   - **Recall**: Importante para minimizar falsos negativos\n",
    "   - **F1-Score**: Balan√ßo entre Precision e Recall\n",
    "   - **ROC-AUC**: Capacidade de discrimina√ß√£o do modelo\n",
    "\n",
    "3. **Arquiteturas Otimizadas**:\n",
    "   - **BinaryCNN_Light**: R√°pido e eficiente para infer√™ncia em tempo real\n",
    "   - **BinaryCNN_Deep**: Maior capacidade com conex√µes residuais\n",
    "   - **BinaryCNN_Efficient**: Balan√ßo entre velocidade e acur√°cia\n",
    "\n",
    "4. **T√©cnicas Avan√ßadas**:\n",
    "   - Data Augmentation (Mixup, CutMix)\n",
    "   - Learning Rate Scheduling\n",
    "   - Early Stopping\n",
    "   - Mixed Precision Training\n",
    "   - Visualiza√ß√£o detalhada de progresso\n",
    "\n",
    "### Vantagens da Classifica√ß√£o Bin√°ria:\n",
    "- ‚úÖ Maior acur√°cia esperada\n",
    "- ‚úÖ Mais simples de interpretar\n",
    "- ‚úÖ Menos dados necess√°rios para treinamento\n",
    "- ‚úÖ Ideal para triagem inicial\n",
    "- ‚úÖ Deploy mais eficiente\n",
    "\n",
    "### Pr√≥ximos Passos:\n",
    "- An√°lise detalhada das m√©tricas\n",
    "- Testes com imagens reais\n",
    "- Otimiza√ß√£o de hiperpar√¢metros\n",
    "- Prepara√ß√£o para deploy em produ√ß√£o\n",
    "- Integra√ß√£o com sistema de monitoramento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
